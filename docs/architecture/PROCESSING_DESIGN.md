# Post-Capture Processing Design Document

**Feature**: Process Tab - Complete the astro-imaging workflow
**Status**: Design Phase
**Priority**: Medium-High
**Target**: Q2-Q3 2026
**Cost**: $0 (all free/open-source tools)

---

## Table of Contents

1. [Overview](#overview)
2. [User Workflow](#user-workflow)
3. [Seestar S50 Output Analysis](#seestar-s50-output-analysis)
4. [Technical Architecture](#technical-architecture)
5. [Processing Pipeline](#processing-pipeline)
6. [UI/UX Design](#uiux-design)
7. [Free Tools & Libraries](#free-tools--libraries)
8. [Implementation Phases](#implementation-phases)
9. [Storage & Performance](#storage--performance)
10. [Security Considerations](#security-considerations)

---

## Overview

### Vision
Complete the observing workflow with integrated post-processing:
```
Plan â†’ Browse â†’ Observe â†’ Process
```

### Goals
1. **Hybrid Workflow**: Quick presets for common tasks + advanced customization
2. **Free Stack**: Leverage open-source tools (Siril, Astropy, OpenCV)
3. **Session-Aware**: Link processing to observation sessions/plans
4. **Educational**: Teach processing concepts with visual feedback
5. **Export-Friendly**: Prepare files for external tools (PixInsight, Photoshop)

### Non-Goals (Phase 1)
- âŒ Replace professional tools (PixInsight, Photoshop)
- âŒ Advanced operations (deconvolution, HDR, narrowband)
- âŒ Real-time processing during observation
- âŒ Planetary video processing (focus on DSO stacking)

---

## User Workflow

### Scenario 1: Quick Processing (Beginner)
```
1. User uploads session files (stacked FITS from Seestar)
2. System detects session metadata (target, exposure, filter)
3. User clicks "Quick Process" â†’ Auto-applies preset
4. System runs: Calibration â†’ Stretch â†’ Color Balance
5. User previews result, downloads JPEG/TIFF
6. Optional: "Export for PixInsight" (16-bit TIFF)
```

### Scenario 2: Custom Workflow (Advanced)
```
1. User uploads session files
2. User builds custom pipeline:
   - Add "Gradient Removal" step
   - Add "Star Reduction" step
   - Add "Histogram Stretch" with custom black point
3. System shows preview at each step
4. User adjusts parameters, re-runs steps
5. User saves workflow as "recipe" for future sessions
6. Downloads final image + processing log
```

### Scenario 3: Session Analysis
```
1. User uploads session files
2. System analyzes:
   - FWHM/seeing quality per frame
   - Star counts and distribution
   - Background noise levels
   - Success rate (good frames vs rejected)
3. User sees session report with recommendations
4. User decides to re-process with different settings
```

---

## Seestar S50 Output Analysis

### File Formats Produced

**Primary Output** (need confirmation from Seestar docs):
- **Stacked FITS**: `M31_stacked.fit` - Main output after live stacking
- **Individual Lights**: `M31_light_001.fit`, `M31_light_002.fit`, etc.
- **Calibration Frames**:
  - Dark frames: `dark_10s_001.fit`
  - Flat frames: `flat_001.fit`
  - Bias frames: `bias_001.fit`
- **Session Log**: `M31_session.json` - Metadata (start time, coordinates, settings)
- **Preview JPEG**: `M31_preview.jpg` - Quick preview for mobile

**Metadata in FITS Headers**:
```
OBJECT = 'M31'
EXPTIME = 10.0
FILTER = 'L'
INSTRUME = 'Seestar S50'
RA = 0.712
DEC = 41.269
DATE-OBS = '2025-11-05T02:30:00'
IMAGETYP = 'Light Frame'
```

### Typical Session Structure
```
session_2025-11-05_M31/
â”œâ”€â”€ M31_stacked.fit           # Main stacked image (16-bit)
â”œâ”€â”€ M31_preview.jpg           # Quick preview (8-bit JPEG)
â”œâ”€â”€ lights/
â”‚   â”œâ”€â”€ M31_light_001.fit     # Individual 10s exposures
â”‚   â”œâ”€â”€ M31_light_002.fit
â”‚   â””â”€â”€ ... (100 frames)
â”œâ”€â”€ calibration/
â”‚   â”œâ”€â”€ darks/
â”‚   â”‚   â””â”€â”€ dark_10s_001.fit
â”‚   â”œâ”€â”€ flats/
â”‚   â”‚   â””â”€â”€ flat_001.fit
â”‚   â””â”€â”€ bias/
â”‚       â””â”€â”€ bias_001.fit
â””â”€â”€ session_log.json          # Metadata
```

---

## Technical Architecture

### Backend Components

```python
# New services to add

backend/app/services/
â”œâ”€â”€ processing_service.py      # Main processing orchestrator
â”œâ”€â”€ fits_handler.py            # FITS file I/O, header parsing
â”œâ”€â”€ calibration_service.py     # Dark/flat/bias application
â”œâ”€â”€ stacking_service.py        # Stack individual frames (if needed)
â”œâ”€â”€ stretch_service.py         # Histogram stretching
â”œâ”€â”€ gradient_service.py        # Gradient/vignetting removal
â”œâ”€â”€ pipeline_builder.py        # Custom workflow builder
â””â”€â”€ session_analyzer.py        # Quality metrics, FWHM, SNR

backend/app/models/
â”œâ”€â”€ processing_models.py       # Job, Pipeline, Step models
â””â”€â”€ session_models.py          # Session, Frame, Quality models
```

### Database Schema

```sql
-- Processing sessions
CREATE TABLE processing_sessions (
    id INTEGER PRIMARY KEY,
    user_id INTEGER,  -- Future: multi-user support
    session_name VARCHAR(100),
    observation_plan_id INTEGER,  -- Link to original plan
    upload_timestamp DATETIME,
    total_files INTEGER,
    total_size_bytes BIGINT,
    status VARCHAR(20),  -- 'uploading', 'ready', 'processing', 'complete', 'error'
    metadata JSON,  -- Session log, target info
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Individual files in session
CREATE TABLE processing_files (
    id INTEGER PRIMARY KEY,
    session_id INTEGER REFERENCES processing_sessions(id),
    filename VARCHAR(255),
    file_type VARCHAR(20),  -- 'light', 'dark', 'flat', 'bias', 'stacked'
    file_path VARCHAR(500),
    file_size_bytes BIGINT,
    exposure_seconds FLOAT,
    filter_name VARCHAR(10),
    temperature_celsius FLOAT,
    quality_score FLOAT,  -- FWHM, star count, etc.
    metadata JSON,  -- FITS headers
    uploaded_at DATETIME
);

-- Processing jobs
CREATE TABLE processing_jobs (
    id INTEGER PRIMARY KEY,
    session_id INTEGER REFERENCES processing_sessions(id),
    pipeline_id INTEGER REFERENCES processing_pipelines(id),
    status VARCHAR(20),  -- 'queued', 'running', 'complete', 'failed'
    progress_percent FLOAT,
    current_step VARCHAR(50),
    started_at DATETIME,
    completed_at DATETIME,
    output_files JSON,  -- List of generated files
    error_message TEXT,
    processing_log TEXT
);

-- Processing pipelines (saved workflows)
CREATE TABLE processing_pipelines (
    id INTEGER PRIMARY KEY,
    user_id INTEGER,
    name VARCHAR(100),
    description TEXT,
    pipeline_steps JSON,  -- Array of step configurations
    is_preset BOOLEAN DEFAULT FALSE,
    created_at DATETIME,
    updated_at DATETIME
);

-- Processing steps (individual operations)
CREATE TABLE processing_steps (
    id INTEGER PRIMARY KEY,
    pipeline_id INTEGER REFERENCES processing_pipelines(id),
    step_order INTEGER,
    step_type VARCHAR(50),  -- 'calibrate', 'stack', 'stretch', 'gradient', etc.
    parameters JSON,  -- Step-specific params
    enabled BOOLEAN DEFAULT TRUE
);
```

### Processing Queue Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI     â”‚â”€â”€â”€â”€â†’â”‚  Redis Queue  â”‚â”€â”€â”€â”€â†’â”‚   Celery Worker      â”‚
â”‚  /process    â”‚     â”‚   (Job Queue) â”‚     â”‚   (Orchestrator)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚  Docker Container       â”‚
                                           â”‚  (Per Job)              â”‚
                                           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                           â”‚  â”‚ Processing        â”‚  â”‚
                                           â”‚  â”‚ Pipeline          â”‚  â”‚
                                           â”‚  â”‚ (Siril/Astropy)   â”‚  â”‚
                                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                           â”‚  Limits: 4GB RAM/2 CPU  â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚  Shared Volume          â”‚
                                           â”‚  /data/processing/      â”‚
                                           â”‚  - Input FITS           â”‚
                                           â”‚  - Output TIFF/JPEG     â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why async processing?**
- FITS stacking can take 5-30 minutes
- User shouldn't wait with browser open
- WebSocket updates for progress
- Background workers scale independently

**Why Docker containers per job?**
- **Isolation**: Each job runs in isolated environment
- **Security**: No access to host filesystem beyond mounted volumes
- **Resource limits**: Enforced memory/CPU caps per container
- **Clean state**: Fresh environment for each job, no leftover artifacts
- **Portability**: Consistent environment across dev/staging/prod
- **Easy cleanup**: Container auto-removed after job completes

---

## Processing Pipeline

### Quick Presets (Phase 1)

#### 1. Quick DSO Process (Beginner)
```python
pipeline_quick_dso = [
    {
        "step": "calibrate",
        "params": {
            "apply_darks": True,
            "apply_flats": True,
            "apply_bias": True
        }
    },
    {
        "step": "histogram_stretch",
        "params": {
            "algorithm": "auto",  # Auto-detect black/white points
            "midtones": 0.5
        }
    },
    {
        "step": "color_balance",
        "params": {
            "algorithm": "auto"  # Auto white balance
        }
    },
    {
        "step": "export",
        "params": {
            "format": "jpeg",
            "quality": 95,
            "bit_depth": 8
        }
    }
]
```

#### 2. Export for External Tool
```python
pipeline_export = [
    {
        "step": "calibrate",
        "params": {
            "apply_darks": True,
            "apply_flats": True,
            "apply_bias": True
        }
    },
    {
        "step": "export",
        "params": {
            "format": "tiff",
            "bit_depth": 16,  # Preserve dynamic range
            "compression": "none"
        }
    }
]
```

### Advanced Steps (Phase 2)

#### 3. Gradient Removal
```python
{
    "step": "remove_gradient",
    "params": {
        "algorithm": "dbf",  # Dynamic Background Extraction
        "smoothness": 0.5,
        "samples": 50
    }
}
```

#### 4. Star Reduction
```python
{
    "step": "star_reduction",
    "params": {
        "strength": 0.5,  # 0-1 scale
        "preserve_brightness": True,
        "halo_reduction": True
    }
}
```

#### 5. Noise Reduction
```python
{
    "step": "denoise",
    "params": {
        "algorithm": "non_local_means",
        "strength": 5.0,
        "luminance_only": True
    }
}
```

### Processing Pipeline Example Code

```python
# backend/app/services/processing_service.py

from astropy.io import fits
import numpy as np
import subprocess
import tempfile
from pathlib import Path

class ProcessingService:
    """Orchestrates processing pipeline execution."""

    def __init__(self):
        self.siril_path = "/usr/bin/siril-cli"  # Siril command-line
        self.temp_dir = Path("/tmp/astro_processing")

    async def execute_pipeline(
        self,
        session_id: int,
        pipeline_id: int,
        job_id: int
    ) -> dict:
        """Execute a processing pipeline on a session."""

        # Load session files
        session = await self.get_session(session_id)
        pipeline = await self.get_pipeline(pipeline_id)

        # Create working directory
        work_dir = self.temp_dir / f"job_{job_id}"
        work_dir.mkdir(parents=True, exist_ok=True)

        # Execute each step in pipeline
        current_file = session.stacked_file_path

        for step in pipeline.steps:
            if step.step_type == "calibrate":
                current_file = await self.apply_calibration(
                    current_file,
                    session,
                    step.parameters,
                    work_dir
                )

            elif step.step_type == "histogram_stretch":
                current_file = await self.stretch_histogram(
                    current_file,
                    step.parameters,
                    work_dir
                )

            elif step.step_type == "remove_gradient":
                current_file = await self.remove_gradient(
                    current_file,
                    step.parameters,
                    work_dir
                )

            elif step.step_type == "export":
                output_file = await self.export_image(
                    current_file,
                    step.parameters,
                    work_dir
                )

            # Update job progress
            await self.update_job_progress(job_id, step.step_order)

        return {
            "status": "complete",
            "output_file": output_file,
            "processing_log": self.get_log(work_dir)
        }

    async def apply_calibration(
        self,
        light_file: Path,
        session,
        params: dict,
        work_dir: Path
    ) -> Path:
        """Apply dark/flat/bias calibration using Siril."""

        # Create Siril script
        script = work_dir / "calibrate.ssf"
        script.write_text(f"""
requires 1.2.0
cd {work_dir}
convert light -out={work_dir}
preprocess light -dark={session.master_dark} -flat={session.master_flat} -bias={session.master_bias} -cc=dark -cc=bias -cfa -equalize_cfa
""")

        # Run Siril
        result = subprocess.run(
            [self.siril_path, "-s", str(script)],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            raise ProcessingError(f"Calibration failed: {result.stderr}")

        return work_dir / "pp_light.fit"

    async def stretch_histogram(
        self,
        input_file: Path,
        params: dict,
        work_dir: Path
    ) -> Path:
        """Stretch histogram using custom algorithm."""

        # Load FITS
        with fits.open(input_file) as hdul:
            data = hdul[0].data.astype(np.float32)
            header = hdul[0].header

        # Apply stretch
        if params["algorithm"] == "auto":
            # Auto-detect black/white points
            black_point = np.percentile(data, 0.1)
            white_point = np.percentile(data, 99.9)
        else:
            black_point = params.get("black_point", 0)
            white_point = params.get("white_point", 65535)

        # Linear stretch
        stretched = np.clip((data - black_point) / (white_point - black_point), 0, 1)

        # Apply midtones transfer function
        midtones = params.get("midtones", 0.5)
        stretched = self.mtf(stretched, midtones)

        # Save stretched FITS
        output_file = work_dir / "stretched.fit"
        fits.writeto(output_file, stretched, header, overwrite=True)

        return output_file

    def mtf(self, data: np.ndarray, midtones: float) -> np.ndarray:
        """Midtones Transfer Function (non-linear stretch)."""
        return (midtones - 1) * data / ((2 * midtones - 1) * data - midtones)
```

---

## UI/UX Design

### Process Tab Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸŽ¨ Process                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Upload Session     â”‚    â”‚  Recent Sessions             â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  Drag & drop FITS   â”‚    â”‚  â”‚ M31 - Nov 5, 2025     â”‚  â”‚  â”‚
â”‚  â”‚  files here         â”‚    â”‚  â”‚ 150 frames, 4.2GB     â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  â”‚ [Quick Process] [View]â”‚  â”‚  â”‚
â”‚  â”‚  Or [Browse Files]  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  Supports:          â”‚    â”‚  â”‚ M42 - Nov 4, 2025     â”‚  â”‚  â”‚
â”‚  â”‚  â€¢ FITS (.fit)      â”‚    â”‚  â”‚ 200 frames, 5.8GB     â”‚  â”‚  â”‚
â”‚  â”‚  â€¢ Seestar sessions â”‚    â”‚  â”‚ [Quick Process] [View]â”‚  â”‚  â”‚
â”‚  â”‚  â€¢ ZIP archives     â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Session View (After Upload)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â† Back to Sessions    Session: M31 - Nov 5, 2025              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Session Info                                             â”‚  â”‚
â”‚  â”‚ Target: M31 (Andromeda Galaxy)                           â”‚  â”‚
â”‚  â”‚ Frames: 150 lights + 20 darks + 10 flats                 â”‚  â”‚
â”‚  â”‚ Exposure: 10s per frame                                  â”‚  â”‚
â”‚  â”‚ Total Integration: 25 minutes                            â”‚  â”‚
â”‚  â”‚ Stacked: Yes (M31_stacked.fit)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Quick Presets      â”‚    â”‚  Preview                     â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  [Quick DSO]        â”‚    â”‚  â”‚                        â”‚  â”‚  â”‚
â”‚  â”‚  [Deep Stretch]     â”‚    â”‚  â”‚   [Image Preview]      â”‚  â”‚  â”‚
â”‚  â”‚  [Export for PI]    â”‚    â”‚  â”‚                        â”‚  â”‚  â”‚
â”‚  â”‚  [Custom Pipeline]  â”‚    â”‚  â”‚                        â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  Or build custom:   â”‚    â”‚  Original FITS              â”‚  â”‚
â”‚  â”‚  [+ Add Step â–¼]     â”‚    â”‚                              â”‚  â”‚
â”‚  â”‚                     â”‚    â”‚  [Download] [Share]          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Session Analysis                                         â”‚  â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚
â”‚  â”‚ FWHM (Seeing): 2.1" (Good)                  â­â­â­â­â˜†   â”‚  â”‚
â”‚  â”‚ Star Count: 12,458                                       â”‚  â”‚
â”‚  â”‚ Background Noise: Low                                    â”‚  â”‚
â”‚  â”‚ Rejected Frames: 5 (3.3%)                                â”‚  â”‚
â”‚  â”‚ [View Detailed Report]                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Custom Pipeline Builder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â† Back to Session    Custom Pipeline Builder                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Pipeline Steps                         Preview                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                             â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚  1. âœ“ Calibration           â”‚    â”‚   [Before/After     â”‚  â”‚
â”‚  â”‚     [âš™ï¸ Settings] [ðŸ—‘ï¸]      â”‚    â”‚    Slider Preview]  â”‚  â”‚
â”‚  â”‚                             â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚  2. â¯ï¸ Histogram Stretch     â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚     Black: â–¬â–¬â–¬â—â”€â”€â”€â”€â”€â”€ 512   â”‚    â”‚  Step 2: Stretch    â”‚  â”‚
â”‚  â”‚     White: â”€â”€â”€â”€â”€â—â–¬â–¬â–¬ 45000  â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚     Mid:   â”€â”€â”€â—â”€â”€â”€â”€â”€ 0.5    â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚     [âš™ï¸ Settings] [ðŸ—‘ï¸]      â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚                             â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚  3. â¸ï¸ Gradient Removal      â”‚    â”‚  [Apply Changes]    â”‚  â”‚
â”‚  â”‚     [âš™ï¸ Settings] [ðŸ—‘ï¸]      â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚                             â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  [+ Add Step â–¼]             â”‚                              â”‚
â”‚  â”‚     Calibration             â”‚    Processing Time: ~8 min   â”‚
â”‚  â”‚     Histogram Stretch       â”‚                              â”‚
â”‚  â”‚     Gradient Removal        â”‚    [â–¶ï¸ Run Pipeline]         â”‚
â”‚  â”‚     Star Reduction          â”‚    [ðŸ’¾ Save as Preset]       â”‚
â”‚  â”‚     Color Balance           â”‚                              â”‚
â”‚  â”‚     Denoise                 â”‚                              â”‚
â”‚  â”‚     Export                  â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing in Progress

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processing: M31 - Quick DSO Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Progress                                                â”‚  â”‚
â”‚  â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  60%                    â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Current Step: Histogram Stretch (Step 2 of 4)          â”‚  â”‚
â”‚  â”‚  Elapsed: 4m 32s                                         â”‚  â”‚
â”‚  â”‚  Estimated Remaining: 3m 08s                             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  âœ“ Step 1: Calibration (Complete)                       â”‚  â”‚
â”‚  â”‚  â³ Step 2: Histogram Stretch (Running...)               â”‚  â”‚
â”‚  â”‚  â¸ï¸ Step 3: Color Balance (Pending)                      â”‚  â”‚
â”‚  â”‚  â¸ï¸ Step 4: Export (Pending)                             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  [Cancel Processing]                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  Processing Log:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ [12:34:56] Starting calibration...                       â”‚  â”‚
â”‚  â”‚ [12:35:12] Applied 20 dark frames                        â”‚  â”‚
â”‚  â”‚ [12:35:45] Applied 10 flat frames                        â”‚  â”‚
â”‚  â”‚ [12:36:02] Calibration complete                          â”‚  â”‚
â”‚  â”‚ [12:36:15] Starting histogram stretch...                 â”‚  â”‚
â”‚  â”‚ [12:36:18] Auto black point: 512                         â”‚  â”‚
â”‚  â”‚ [12:36:19] Auto white point: 45234                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Free Tools & Libraries

### Core Processing Stack (Backend)

#### 1. Siril (Command-Line)
**Purpose**: Stacking, calibration, registration
**License**: GPL-3.0 (Free)
**Installation**: `apt install siril`
**Why**: Industry-standard free tool, command-line capable

```bash
# Example Siril script for calibration
cd /tmp/session
convert light -out=/tmp/session
preprocess light -dark=master_dark -flat=master_flat -cfa
```

#### 2. Astropy
**Purpose**: FITS I/O, header manipulation, coordinates
**License**: BSD (Free)
**Installation**: `pip install astropy`
**Why**: Standard Python astronomy library

```python
from astropy.io import fits
from astropy.wcs import WCS

# Read FITS
with fits.open('M31_stacked.fit') as hdul:
    data = hdul[0].data
    header = hdul[0].header
    wcs = WCS(header)

# Modify and save
header['PROCESSED'] = 'True'
fits.writeto('M31_processed.fit', data, header)
```

#### 3. OpenCV
**Purpose**: Image processing, noise reduction
**License**: Apache-2.0 (Free)
**Installation**: `pip install opencv-python`
**Why**: Fast, optimized computer vision

```python
import cv2

# Denoise
denoised = cv2.fastNlMeansDenoising(image, h=10)

# Gradient removal (background subtraction)
background = cv2.blur(image, (50, 50))
gradient_removed = cv2.subtract(image, background)
```

#### 4. scikit-image
**Purpose**: Advanced image processing
**License**: BSD (Free)
**Installation**: `pip install scikit-image`
**Why**: Academic-quality algorithms

```python
from skimage import exposure

# Histogram stretch
stretched = exposure.rescale_intensity(
    image,
    in_range=(black_point, white_point)
)

# Adaptive histogram equalization
equalized = exposure.equalize_adapthist(image)
```

#### 5. NumPy
**Purpose**: Array operations, mathematical transforms
**License**: BSD (Free)
**Installation**: `pip install numpy`
**Why**: Foundation of scientific Python

```python
import numpy as np

# Median combine (for master calibration)
master_dark = np.median(dark_frames, axis=0)

# Statistics
mean = np.mean(image)
std = np.std(image)
snr = mean / std
```

### Frontend Libraries

#### 1. FITS.js or fits-viewer.js
**Purpose**: Display FITS in browser
**License**: MIT (Free)
**Why**: View FITS without conversion

```javascript
// Display FITS in canvas
const fits = new FITS();
fits.load(url, function() {
  fits.displayTo('#canvas');
});
```

#### 2. Chart.js
**Purpose**: Histogram visualization
**License**: MIT (Free)
**Why**: Interactive charts for data analysis

```javascript
// Display histogram
new Chart(ctx, {
  type: 'bar',
  data: {
    labels: bins,
    datasets: [{
      label: 'Pixel Distribution',
      data: histogram
    }]
  }
});
```

---

## Implementation Phases

### Phase 1: MVP (Q2 2026) - 4-6 weeks

**Goal**: Basic upload, quick preset, download

**Features**:
- âœ… File upload (FITS, ZIP)
- âœ… Session parsing and display
- âœ… One quick preset: "Quick DSO"
- âœ… Calibration (dark/flat/bias)
- âœ… Histogram stretch
- âœ… Export JPEG/TIFF
- âœ… Basic preview
- âœ… **Docker containerization for processing**
- âœ… **Resource limits and security hardening**

**Dependencies**:
```python
# Main application
astropy==6.0.0
opencv-python==4.8.0
pillow==10.0.0
numpy==1.24.0
celery==5.3.0
redis==5.0.0
docker==7.0.0  # Docker Python SDK
websockets==12.0  # For progress updates
```

**Docker Setup**:
```bash
# Build processing worker image
docker build -f docker/processing-worker.Dockerfile -t astro-planner/processing-worker:latest .

# Start services
docker-compose up -d

# Scale celery workers (if needed)
docker-compose up -d --scale celery-worker=3
```

**API Endpoints**:
```
POST   /api/process/upload          # Upload session files
GET    /api/process/sessions        # List sessions
GET    /api/process/sessions/{id}   # Get session details
POST   /api/process/sessions/{id}/quick  # Run quick preset
GET    /api/process/jobs/{id}       # Get job status
POST   /api/process/jobs/{id}/cancel # Cancel running job
GET    /api/process/jobs/{id}/download  # Download result
WS     /ws/process/jobs/{id}        # WebSocket for real-time updates
```

**Database Migrations**:
```bash
# Create processing tables
alembic revision -m "Add processing tables"
alembic upgrade head
```

### Phase 2: Custom Pipelines (Q3 2026) - 4-6 weeks

**Goal**: Advanced workflow builder

**Features**:
- âœ… Custom pipeline builder UI
- âœ… 5+ processing steps
- âœ… Parameter adjustment
- âœ… Step-by-step preview
- âœ… Save/load recipes
- âœ… Before/after comparison

**New Steps**:
- Gradient removal
- Star reduction
- Noise reduction
- Color balance
- Sharpening

### Phase 3: Analysis & Integration (Q4 2026) - 4-6 weeks

**Goal**: Session analysis and plan integration

**Features**:
- âœ… FWHM/seeing analysis
- âœ… Star count statistics
- âœ… Frame quality scoring
- âœ… Session report generation
- âœ… Link to observation plan
- âœ… Batch processing multiple sessions
- âœ… Export to PixInsight format

**New Services**:
```python
session_analyzer.py     # Quality metrics
star_detection.py       # SExtractor integration
plate_solver.py         # Astrometry.net (optional)
```

---

## Storage & Performance

### File Storage Strategy

**Phase 1**: Local filesystem
```
/app/data/processing/
â”œâ”€â”€ sessions/
â”‚   â”œâ”€â”€ session_001/
â”‚   â”‚   â”œâ”€â”€ uploads/       # Original files
â”‚   â”‚   â”œâ”€â”€ working/       # Intermediate files
â”‚   â”‚   â””â”€â”€ outputs/       # Final processed files
â”‚   â””â”€â”€ session_002/
â””â”€â”€ cache/
    â””â”€â”€ previews/          # JPEG previews
```

**Phase 2**: S3-compatible storage (MinIO/AWS S3)
- Cheaper long-term storage
- CDN integration for downloads
- Automatic cleanup after 30 days

### Performance Considerations

**File Size Limits**:
- Single file: 500MB max
- Session total: 10GB max
- Automatic ZIP extraction

**Processing Timeouts**:
- Quick preset: 5 minutes max
- Custom pipeline: 30 minutes max
- Container timeout: 1 hour (hard limit)
- Idle container cleanup: 15 minutes

**Docker Resource Management**:
- Container pool: Pre-warmed containers for faster startup
- Concurrent jobs: Max 5 processing containers simultaneously
- CPU throttling: 2 cores per container
- Memory limit: 4GB per container with OOM killer enabled
- Disk I/O: Limited to mounted volumes only

**Optimization**:
- Downsampled previews (512x512) for real-time UI
- Progressive JPEG for fast loading
- Redis caching for session metadata
- Cleanup old sessions after 30 days
- **Container image caching**: Keep processing image pulled
- **Volume reuse**: Mount job directories efficiently
- **Parallel processing**: Multiple containers for batch jobs

---

## Security Considerations

### File Upload Safety

1. **Validation**:
   - Allowed extensions: `.fit`, `.fits`, `.fit.gz`, `.zip`
   - Magic number validation (not just extension)
   - Maximum file size enforcement
   - Virus scanning (ClamAV) for uploaded files

2. **Sandboxing via Docker**:
   - Each job runs in isolated Docker container
   - No access to host filesystem (only mounted volumes)
   - Network disabled for processing containers
   - No shell command injection (use subprocess with lists)
   - Containers run as non-root user (UID 1000)
   - Read-only root filesystem
   - All capabilities dropped (no privilege escalation)
   - Container auto-removed after completion

3. **Resource Limits (Docker-Enforced)**:
   - Memory limit per job (4GB) - hard limit with OOM killer
   - CPU limit per job (2 cores) - enforced via cgroups
   - Max 100 processes per container
   - Max 1024 open files per container
   - Disk I/O limited to mounted volumes
   - Job queue size limits (max 20 queued jobs)
   - Timeout enforcement (30 min default, 1 hour max)

### Privacy & Data Retention

1. **Data Ownership**:
   - Users own uploaded data
   - No training on user data
   - Optional: User accounts for privacy

2. **Retention Policy**:
   - Uploaded files: 7 days
   - Processed files: 30 days
   - Session metadata: 90 days
   - User can delete anytime

3. **Access Control**:
   - Session IDs are UUIDs (non-guessable)
   - No public listing of sessions
   - Optional: Authentication for sensitive data

---

## Docker Containerization

### Processing Worker Docker Image

**Dockerfile** (`docker/processing-worker.Dockerfile`):
```dockerfile
# Use NVIDIA CUDA base image for GPU acceleration
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

# Install Python 3.11
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install system dependencies
RUN apt-get update && apt-get install -y \
    siril \
    libopencv-dev \
    && rm -rf /var/lib/apt/lists/*

# Set up working directory
WORKDIR /app

# Install Python dependencies (GPU-accelerated versions)
COPY requirements-processing.txt .
RUN pip install --no-cache-dir -r requirements-processing.txt

# Copy processing scripts
COPY backend/app/services/processing/ /app/processing/
COPY backend/app/models/ /app/models/

# Create processing user (non-root for security)
RUN useradd -m -u 1000 processor && \
    chown -R processor:processor /app

USER processor

# Entry point for processing jobs
ENTRYPOINT ["python3.11", "-m", "processing.runner"]
```

**requirements-processing.txt** (with GPU support):
```txt
# Core astronomy libraries
astropy==6.0.0

# GPU-accelerated image processing
opencv-contrib-python==4.8.0  # Includes CUDA modules
cupy-cuda12x==13.0.0          # GPU-accelerated NumPy replacement
pillow==10.0.0
numpy==1.24.0
scikit-image==0.21.0
scipy==1.11.0

# GPU-accelerated deep learning (for denoising, etc.)
torch==2.1.0+cu121           # PyTorch with CUDA 12.1
torchvision==0.16.0+cu121

# Optional: GPU-accelerated signal processing
cucim==23.10.0               # RAPIDS image processing
```

### Docker Compose Configuration

**docker-compose.yml** (updated with GPU support):
```yaml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data/processing:/data/processing
      - /var/run/docker.sock:/var/run/docker.sock  # Docker-in-Docker
    environment:
      - REDIS_URL=redis://redis:6379
      - DOCKER_HOST=unix:///var/run/docker.sock
    depends_on:
      - redis
      - db

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

  celery-worker:
    build: .
    command: celery -A backend.app.celery worker --loglevel=info
    volumes:
      - ./data/processing:/data/processing
      - /var/run/docker.sock:/var/run/docker.sock  # Docker-in-Docker
    environment:
      - REDIS_URL=redis://redis:6379
      - NVIDIA_VISIBLE_DEVICES=all  # Expose all GPUs to worker
    depends_on:
      - redis

  db:
    image: postgres:15-alpine
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=astro_pass
      - POSTGRES_DB=astro_planner

volumes:
  redis-data:
  db-data:
```

**System Requirements**:
```bash
# Install NVIDIA Container Toolkit (one-time setup)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# Verify GPU access
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### Processing Service with Docker

**backend/app/services/processing_service.py** (updated):
```python
import docker
import tempfile
from pathlib import Path
from typing import Dict, Any
import json

class ProcessingService:
    """Orchestrates processing pipeline execution using Docker containers."""

    def __init__(self):
        self.docker_client = docker.from_env()
        self.processing_image = "astro-planner/processing-worker:latest"
        self.data_dir = Path("/data/processing")

    async def execute_pipeline(
        self,
        session_id: int,
        pipeline_id: int,
        job_id: int
    ) -> Dict[str, Any]:
        """Execute a processing pipeline in an isolated Docker container."""

        # Load session and pipeline
        session = await self.get_session(session_id)
        pipeline = await self.get_pipeline(pipeline_id)

        # Create job-specific directory
        job_dir = self.data_dir / f"job_{job_id}"
        job_dir.mkdir(parents=True, exist_ok=True)

        # Prepare job configuration
        job_config = {
            "session_id": session_id,
            "pipeline_id": pipeline_id,
            "job_id": job_id,
            "input_file": str(session.stacked_file_path),
            "output_dir": str(job_dir / "outputs"),
            "pipeline_steps": pipeline.steps
        }

        config_file = job_dir / "job_config.json"
        config_file.write_text(json.dumps(job_config))

        try:
            # Run processing in Docker container with GPU support
            container = self.docker_client.containers.run(
                image=self.processing_image,
                command=["--config", f"/job/job_config.json"],
                volumes={
                    str(job_dir): {"bind": "/job", "mode": "rw"}
                },
                environment={
                    "JOB_ID": str(job_id),
                    "NVIDIA_VISIBLE_DEVICES": "all",  # Enable GPU access
                    "CUDA_VISIBLE_DEVICES": "0"       # Use first GPU (or all: "0,1,2,3")
                },
                # GPU configuration
                device_requests=[
                    docker.types.DeviceRequest(
                        count=-1,  # Use all GPUs (-1) or specify count
                        capabilities=[['gpu', 'compute', 'utility']]
                    )
                ],
                # Resource limits
                mem_limit="4g",
                memswap_limit="4g",
                cpu_quota=200000,  # 2 CPU cores
                # Security
                network_disabled=True,  # No network access needed
                read_only=False,  # Need write for output
                security_opt=["no-new-privileges"],
                # Cleanup
                auto_remove=True,
                detach=False,  # Wait for completion
                # Logging
                stdout=True,
                stderr=True
            )

            # Container completed successfully
            logs = container.decode('utf-8')

            # Parse output
            output_file = job_dir / "outputs" / "final.tiff"

            return {
                "status": "complete",
                "output_file": str(output_file),
                "processing_log": logs
            }

        except docker.errors.ContainerError as e:
            # Processing failed
            return {
                "status": "failed",
                "error": str(e),
                "logs": e.stderr.decode('utf-8')
            }

        except docker.errors.ImageNotFound:
            raise Exception(f"Processing image not found: {self.processing_image}")

        except docker.errors.APIError as e:
            raise Exception(f"Docker API error: {str(e)}")

    async def build_processing_image(self):
        """Build the processing worker Docker image."""

        self.docker_client.images.build(
            path=".",
            dockerfile="docker/processing-worker.Dockerfile",
            tag=self.processing_image,
            rm=True
        )
```

### Container Resource Limits

**Per-Job Limits**:
```python
container_limits = {
    "mem_limit": "4g",        # Maximum 4GB RAM
    "memswap_limit": "4g",    # No swap allowed
    "cpu_quota": 200000,      # 2.0 CPU cores (200000/100000)
    "pids_limit": 100,        # Max 100 processes
    "ulimits": [
        docker.types.Ulimit(name='nofile', soft=1024, hard=2048),  # Open files
        docker.types.Ulimit(name='nproc', soft=100, hard=100),     # Processes
    ]
}
```

**Timeout Enforcement**:
```python
import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds: int):
    """Context manager to timeout container execution."""
    def timeout_handler(signum, frame):
        raise TimeoutError(f"Processing exceeded {seconds}s timeout")

    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)

# Usage in celery task
@celery_app.task
def process_session_task(session_id: int, pipeline_id: int, job_id: int):
    """Celery task that launches Docker container."""

    service = ProcessingService()

    with timeout(1800):  # 30 minute timeout
        result = await service.execute_pipeline(
            session_id, pipeline_id, job_id
        )

    return result
```

### Security Hardening

**Container Security Best Practices**:
```python
security_config = {
    # No network access (processing doesn't need internet)
    "network_disabled": True,

    # Drop all capabilities
    "cap_drop": ["ALL"],

    # No privilege escalation
    "security_opt": ["no-new-privileges"],

    # Read-only root filesystem (except mounted volumes)
    "read_only": True,
    "tmpfs": {
        "/tmp": "rw,noexec,nosuid,size=1g"
    },

    # Run as non-root user
    "user": "1000:1000",

    # Prevent container from becoming PID 1
    "init": True
}
```

### Monitoring and Cleanup

**Container Lifecycle Management**:
```python
class ContainerManager:
    """Manages Docker container lifecycle for processing jobs."""

    def __init__(self):
        self.client = docker.from_env()
        self.active_containers = {}

    def start_job(self, job_id: int, config: dict) -> str:
        """Start processing container for job."""

        container = self.client.containers.run(
            image=config["image"],
            command=config["command"],
            volumes=config["volumes"],
            **config["resource_limits"],
            **config["security_config"],
            detach=True,  # Run in background
            name=f"processing-job-{job_id}"
        )

        self.active_containers[job_id] = container.id
        return container.id

    def get_job_status(self, job_id: int) -> dict:
        """Get status of running job."""

        container_id = self.active_containers.get(job_id)
        if not container_id:
            return {"status": "not_found"}

        try:
            container = self.client.containers.get(container_id)
            return {
                "status": container.status,
                "stats": container.stats(stream=False),
                "logs": container.logs(tail=50).decode('utf-8')
            }
        except docker.errors.NotFound:
            return {"status": "not_found"}

    def stop_job(self, job_id: int, timeout: int = 10):
        """Stop and remove container for job."""

        container_id = self.active_containers.get(job_id)
        if not container_id:
            return

        try:
            container = self.client.containers.get(container_id)
            container.stop(timeout=timeout)
            container.remove()
        except docker.errors.NotFound:
            pass
        finally:
            del self.active_containers[job_id]

    def cleanup_old_containers(self, max_age_hours: int = 24):
        """Remove old processing containers."""

        filters = {
            "name": "processing-job-",
            "status": "exited"
        }

        for container in self.client.containers.list(all=True, filters=filters):
            # Check container age
            created = container.attrs["Created"]
            # ... age check logic ...
            container.remove()
```

### GPU-Accelerated Processing Examples

**Example: GPU-accelerated histogram stretch** (`processing/gpu_ops.py`):
```python
import cupy as cp  # GPU-accelerated NumPy
import numpy as np
from astropy.io import fits

def gpu_histogram_stretch(fits_path: str, output_path: str, params: dict):
    """
    GPU-accelerated histogram stretch using CuPy.
    10-50x faster than CPU for large FITS files.
    """

    # Load FITS to CPU
    with fits.open(fits_path) as hdul:
        data = hdul[0].data.astype(np.float32)
        header = hdul[0].header

    # Transfer to GPU
    gpu_data = cp.asarray(data)

    # Calculate percentiles on GPU (much faster)
    black_point = float(cp.percentile(gpu_data, 0.1))
    white_point = float(cp.percentile(gpu_data, 99.9))

    # Apply stretch on GPU
    gpu_stretched = cp.clip(
        (gpu_data - black_point) / (white_point - black_point),
        0, 1
    )

    # Apply midtones transfer function
    midtones = params.get("midtones", 0.5)
    gpu_stretched = (midtones - 1) * gpu_stretched / \
                    ((2 * midtones - 1) * gpu_stretched - midtones)

    # Transfer back to CPU
    stretched = cp.asnumpy(gpu_stretched)

    # Save result
    fits.writeto(output_path, stretched, header, overwrite=True)

    return output_path


def gpu_denoise(image_path: str, output_path: str, strength: float = 10.0):
    """
    GPU-accelerated non-local means denoising using OpenCV CUDA.
    """
    import cv2

    # Load image
    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

    # Upload to GPU
    gpu_img = cv2.cuda_GpuMat()
    gpu_img.upload(img)

    # Denoise on GPU
    gpu_denoised = cv2.cuda.fastNlMeansDenoising(
        gpu_img,
        h=strength,
        searchWindowSize=21,
        blockSize=7
    )

    # Download from GPU
    denoised = gpu_denoised.download()

    # Save result
    cv2.imwrite(output_path, denoised)

    return output_path


def check_gpu_available():
    """Check if GPU is available and return info."""
    try:
        import cupy as cp
        gpu_count = cp.cuda.runtime.getDeviceCount()

        gpu_info = []
        for i in range(gpu_count):
            device = cp.cuda.Device(i)
            props = cp.cuda.runtime.getDeviceProperties(i)

            gpu_info.append({
                "id": i,
                "name": props['name'].decode(),
                "memory_gb": props['totalGlobalMem'] / 1e9,
                "compute_capability": f"{props['major']}.{props['minor']}"
            })

        return {
            "available": True,
            "count": gpu_count,
            "devices": gpu_info
        }
    except Exception as e:
        return {
            "available": False,
            "error": str(e)
        }
```

**Fallback to CPU if GPU unavailable**:
```python
class ProcessingPipeline:
    """Smart pipeline that uses GPU if available, falls back to CPU."""

    def __init__(self):
        self.gpu_info = check_gpu_available()
        self.use_gpu = self.gpu_info["available"]

    def histogram_stretch(self, input_path: str, output_path: str, params: dict):
        """Histogram stretch with automatic GPU/CPU selection."""

        if self.use_gpu:
            try:
                return gpu_histogram_stretch(input_path, output_path, params)
            except Exception as e:
                # GPU failed, fall back to CPU
                print(f"GPU processing failed: {e}. Falling back to CPU.")
                self.use_gpu = False

        # CPU fallback
        return cpu_histogram_stretch(input_path, output_path, params)

    def get_performance_estimate(self, image_size_mb: float) -> dict:
        """Estimate processing time based on GPU availability."""

        if self.use_gpu:
            # GPU is ~20x faster for large images
            time_seconds = image_size_mb * 0.1
            speedup = "20x faster with GPU"
        else:
            time_seconds = image_size_mb * 2.0
            speedup = "CPU mode"

        return {
            "estimated_seconds": time_seconds,
            "speedup": speedup,
            "using_gpu": self.use_gpu
        }
```

### Celery Task with Docker

**backend/app/tasks/processing_tasks.py**:
```python
from celery import Celery
from backend.app.services.processing_service import ProcessingService
from backend.app.services.websocket_service import WebSocketService

celery_app = Celery('tasks', broker='redis://redis:6379/0')

@celery_app.task(bind=True)
def process_session_task(self, session_id: int, pipeline_id: int, job_id: int):
    """
    Celery task that orchestrates Docker container for processing.

    This task runs in the celery worker container and launches
    a separate processing container for actual work.
    """

    service = ProcessingService()
    ws_service = WebSocketService()

    try:
        # Update job status
        await ws_service.send_job_update(job_id, {
            "status": "starting",
            "progress": 0,
            "message": "Launching processing container..."
        })

        # Execute pipeline in Docker container
        result = await service.execute_pipeline(
            session_id, pipeline_id, job_id
        )

        # Send completion notification
        await ws_service.send_job_update(job_id, {
            "status": "complete",
            "progress": 100,
            "output_file": result["output_file"]
        })

        return result

    except Exception as e:
        # Handle failure
        await ws_service.send_job_update(job_id, {
            "status": "failed",
            "error": str(e)
        })
        raise
```

---

## Docker + GPU: Benefits Summary

### Why Docker Containerization?

1. **Security & Isolation**
   - Each job runs in isolated sandbox
   - No access to host filesystem beyond mounted volumes
   - No network access during processing
   - Non-root user execution
   - Automatic cleanup prevents resource leaks

2. **Resource Management**
   - Hard limits on CPU/RAM per job (4GB RAM, 2 CPU cores)
   - Prevents runaway processes
   - Fair resource allocation across concurrent jobs
   - OOM killer prevents system crashes

3. **Reproducibility**
   - Consistent environment across dev/staging/prod
   - Pinned dependency versions
   - Same results regardless of host OS

4. **Scalability**
   - Easy horizontal scaling (spin up more workers)
   - Kubernetes-ready architecture
   - Container orchestration support

### GPU Acceleration Performance Gains

**Typical speedups with NVIDIA GPU**:

| Operation | CPU Time | GPU Time | Speedup |
|-----------|----------|----------|---------|
| Histogram stretch (4K FITS) | 30s | 1.5s | **20x** |
| Noise reduction | 120s | 6s | **20x** |
| Gradient removal | 45s | 3s | **15x** |
| Star detection | 60s | 4s | **15x** |
| Color balance | 15s | 1s | **15x** |

**Full pipeline comparison**:
- **CPU-only**: ~5-8 minutes per session
- **GPU-accelerated**: ~30-60 seconds per session
- **Overall speedup**: ~10-15x faster with GPU

**GPU Requirements**:
- NVIDIA GPU with CUDA 12.x support
- Minimum 4GB VRAM (8GB+ recommended)
- Compute Capability 6.0+ (Pascal or newer)
- Works with consumer GPUs (RTX 3060, 4070, etc.)

**Graceful fallback**:
- System automatically falls back to CPU if GPU unavailable
- No code changes required
- Performance remains acceptable on CPU for small sessions

### Cost-Benefit Analysis

**Infrastructure Costs**:
```
No GPU (CPU-only):
- Server: $50/month (8 CPU cores, 16GB RAM)
- Processing: 5-8 minutes per job
- Throughput: ~10 jobs/hour

With GPU (NVIDIA RTX 4060):
- Server: $80/month (8 CPU cores, 16GB RAM, RTX 4060)
- Processing: 30-60 seconds per job
- Throughput: ~60-120 jobs/hour
- Cost per job: 75% lower
```

**Recommendation**: GPU acceleration is highly recommended for production deployments with >50 jobs/day.

---

## Open Questions & Research Needed

1. **Seestar S50 File Format Confirmation**
   - â“ Exact FITS header structure
   - â“ Does it produce stacked files or only individual frames?
   - â“ Calibration frame availability
   - â“ Session log format
   - **Action**: Test with actual Seestar output

2. **Siril Command-Line Capabilities**
   - â“ Can it be fully automated via scripts?
   - â“ Progress reporting for long operations?
   - â“ Return codes and error handling
   - **Action**: Test Siril CLI in Docker

3. **Frontend FITS Display**
   - â“ Best library for browser FITS viewing?
   - â“ Performance with large files (>100MB)?
   - â“ Stretch/zoom controls
   - **Action**: Prototype with fits-viewer.js

4. **Storage Costs**
   - â“ Average session size from Seestar?
   - â“ How many sessions per user per month?
   - â“ S3 vs local storage breakeven point?
   - **Action**: Estimate based on typical usage

---

## Success Metrics

- **Adoption**: 50% of users try Process tab within first month
- **Completion**: 80% of uploads result in successful processing
- **Speed**: Quick preset completes in <3 minutes (CPU) or <1 minute (GPU)
- **Quality**: 90% user satisfaction with output quality
- **Retention**: Users return to process 3+ sessions per month
- **GPU Utilization**: 70%+ GPU usage during peak hours (if GPU available)
- **Container Efficiency**: <30s container startup time, <5% overhead

---

## Next Steps

1. **Research Phase** (1 week)
   - Test with actual Seestar S50 output files
   - Validate Siril command-line automation in Docker
   - Prototype FITS display in browser
   - **Benchmark GPU vs CPU processing performance**
   - **Test NVIDIA Container Toolkit setup**

2. **Design Review** (1 week)
   - Review with stakeholders
   - Finalize UI mockups
   - Confirm Docker + GPU architecture
   - Resource allocation strategy (CPU vs GPU workers)

3. **Phase 1 Implementation** (4-6 weeks)
   - Build processing worker Docker image (with GPU support)
   - Backend processing service with Docker orchestration
   - File upload and session management
   - Quick DSO preset (GPU-accelerated)
   - Celery worker setup
   - Basic UI with progress tracking

4. **Testing & Iteration** (2 weeks)
   - Test with real session data
   - GPU performance benchmarking
   - Container resource optimization
   - Fallback testing (GPU â†’ CPU)
   - User feedback

---

**Software Cost**: $0/month (all free and open-source!)

**Infrastructure Options**:
- **Phase 1 (Local)**: $0 (local filesystem, CPU-only)
- **Phase 2 (Production, CPU)**: ~$60-70/month
  - Server: $50/month (8 cores, 16GB RAM)
  - Storage: $10-20/month (S3/MinIO)
- **Phase 2 (Production, GPU)**: ~$90-100/month
  - Server: $80/month (8 cores, 16GB RAM, RTX 4060)
  - Storage: $10-20/month (S3/MinIO)
  - **10-15x faster processing with GPU!**

**All processing tools and libraries remain free and open-source!**

---

*Last Updated*: 2025-11-06
*Status*: Design Phase - Docker + GPU Support Added
*Next Action*: Test Docker containerization and GPU acceleration with sample FITS files
